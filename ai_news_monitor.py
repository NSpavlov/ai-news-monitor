#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import requests
import time
from datetime import datetime, timedelta
import openai
import re
import feedparser
from bs4 import BeautifulSoup
import hashlib
import os
import sys
from urllib.parse import urljoin, urlparse
import logging
import aiohttp
import asyncio

# –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–ª—è Windows
if sys.platform.startswith('win'):
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./logs/ai_news.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class AINewsMonitor:
    def __init__(self):
        self.secrets, self.ai_config, self.telegram_config = self.load_config()
        self.openai_client = openai.OpenAI(api_key=self.secrets['OPENAI']['secrets']['API_KEY'])
        self.sent_news_file = './data/sent_news.json'
        self.sent_news = self.load_sent_news()
        
    def load_config(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤—Å—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó"""
        # –®–ª—è—Ö–∏ –¥–ª—è API –∫–ª—é—á—ñ–≤
        if os.path.exists('./config/api_secrets.json'):
            secrets_path = './config/api_secrets.json'
        elif os.path.exists('../.api_secret_infos/api_secrets.json'):
            secrets_path = '../.api_secret_infos/api_secrets.json'
        else:
            raise FileNotFoundError("API secrets file not found")
        # –®–ª—è—Ö–∏ –¥–ª—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π - –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—ñ–∑–Ω—ñ –ª–æ–∫–∞—Ü—ñ—ó
        config_locations = [
            './telegram_config.json',
            './config/telegram_config.json',
            '../telegram_config.json'
        ]
        telegram_config_path = None
        for path in config_locations:
            if os.path.exists(path):
                telegram_config_path = path
                break
        ai_config_locations = [
            './ai_news_config.json',
            './config/ai_news_config.json',
            '../ai_news_config.json'
        ]
        ai_config_path = None
        for path in ai_config_locations:
            if os.path.exists(path):
                ai_config_path = path
                break
        if not telegram_config_path:
            raise FileNotFoundError("telegram_config.json not found")
        if not ai_config_path:
            raise FileNotFoundError("ai_news_config.json not found")
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ñ–∞–π–ª–∏
        with open(secrets_path, 'r', encoding='utf-8') as f:
            secrets = json.load(f)
        with open(ai_config_path, 'r', encoding='utf-8') as f:
            ai_config = json.load(f)
        with open(telegram_config_path, 'r', encoding='utf-8') as f:
            telegram_config = json.load(f)
        return secrets, ai_config, telegram_config
    
    def load_sent_news(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–ø–∏—Å–æ–∫ –≤–∂–µ –Ω–∞–¥—ñ—Å–ª–∞–Ω–∏—Ö –Ω–æ–≤–∏–Ω"""
        if os.path.exists(self.sent_news_file):
            with open(self.sent_news_file, 'r') as f:
                return json.load(f)
        return []
    
    def save_sent_news(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å–ø–∏—Å–æ–∫ –Ω–∞–¥—ñ—Å–ª–∞–Ω–∏—Ö –Ω–æ–≤–∏–Ω"""
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ 1000 –∑–∞–ø–∏—Å—ñ–≤
        if len(self.sent_news) > 1000:
            self.sent_news = self.sent_news[-1000:]
        
        with open(self.sent_news_file, 'w') as f:
            json.dump(self.sent_news, f, indent=2)
    
    def get_news_hash(self, title, url):
        """–°—Ç–≤–æ—Ä—é—î–º–æ —Ö–µ—à –¥–ª—è –Ω–æ–≤–∏–Ω–∏"""
        return hashlib.md5(f"{title}{url}".encode()).hexdigest()
    
    def check_url_validity(self, url, timeout=10):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø—Ä–∞—Ü—é—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è"""
        try:
            response = requests.head(url, timeout=timeout, allow_redirects=True)
            return response.status_code < 400
        except:
            try:
                response = requests.get(url, timeout=timeout, allow_redirects=True)
                return response.status_code < 400
            except:
                return False
    
    def scrape_blog_news(self, url):
        """–ü–∞—Ä—Å–∏–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ –±–ª–æ–≥—ñ–≤"""
        news_items = []
        
        try:
            # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ RSS
            rss_urls = [
                f"{url.rstrip('/')}/feed/",
                f"{url.rstrip('/')}/rss/",
                f"{url.rstrip('/')}/feed.xml",
                f"{url.rstrip('/')}/rss.xml"
            ]
            
            for rss_url in rss_urls:
                try:
                    feed = feedparser.parse(rss_url)
                    if feed.entries:
                        for entry in feed.entries[:5]:  # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ 5
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–æ–≤–∏–Ω–∞ —Å–≤—ñ–∂–∞ (–Ω–µ —Å—Ç–∞—Ä—à–µ 7 –¥–Ω—ñ–≤)
                            if hasattr(entry, 'published_parsed'):
                                pub_date = datetime(*entry.published_parsed[:6])
                                if datetime.now() - pub_date > timedelta(days=7):
                                    continue
                            
                            news_items.append({
                                'title': entry.title,
                                'content': BeautifulSoup(entry.summary, 'html.parser').get_text()[:500],
                                'url': entry.link,
                                'source': urlparse(url).netloc,
                                'published': getattr(entry, 'published', 'Unknown')
                            })
                        break
                except:
                    continue
            
            # –Ø–∫—â–æ RSS –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤, –ø—Ä–æ–±—É—î–º–æ –ø–∞—Ä—Å–∏—Ç–∏ HTML
            if not news_items:
                response = requests.get(url, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –®—É–∫–∞—î–º–æ —Å—Ç–∞—Ç—Ç—ñ –∑–∞ —Ç–∏–ø–æ–≤–∏–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
                articles = soup.find_all(['article', 'div'], class_=re.compile(r'post|article|entry|blog'))
                
                for article in articles[:5]:
                    title_elem = article.find(['h1', 'h2', 'h3'], class_=re.compile(r'title|heading'))
                    link_elem = article.find('a', href=True)
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text().strip()
                        link = urljoin(url, link_elem['href'])
                        
                        # –û—Ç—Ä–∏–º—É—î–º–æ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å
                        content_elem = article.find(['p', 'div'], class_=re.compile(r'excerpt|summary|content'))
                        content = content_elem.get_text().strip()[:500] if content_elem else title
                        
                        news_items.append({
                            'title': title,
                            'content': content,
                            'url': link,
                            'source': urlparse(url).netloc,
                            'published': 'Recent'
                        })
        
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {url}: {e}")
        
        return news_items
    
    async def scrape_blog_news_async(self, session, url):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–≥—É"""
        try:
            async with session.get(url, timeout=15) as response:
                if response.status == 200:
                    content = await response.text()
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–π –∂–µ –∫–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥—É
                    return self.parse_blog_content(content, url)
        except Exception as e:
            logging.error(f"Async error {url}: {e}")
        return []

    def parse_blog_content(self, content, url):
        """–í–∏–¥—ñ–ª–µ–Ω–∏–π –∫–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥—É –∫–æ–Ω—Ç–µ–Ω—Ç—É (–∑ scrape_blog_news)"""
        news_items = []
        try:
            # RSS —Å–ø–æ—á–∞—Ç–∫—É
            import feedparser
            feed = feedparser.parse(content)
            if feed.entries:
                for entry in feed.entries[:5]:
                    if hasattr(entry, 'published_parsed'):
                        pub_date = datetime(*entry.published_parsed[:6])
                        if datetime.now() - pub_date > timedelta(days=7):
                            continue
                    news_items.append({
                        'title': entry.title,
                        'content': BeautifulSoup(entry.summary, 'html.parser').get_text()[:500],
                        'url': entry.link,
                        'source': urlparse(url).netloc,
                        'published': getattr(entry, 'published', 'Unknown')
                    })
            # –Ø–∫—â–æ RSS –Ω–µ –ø—Ä–∞—Ü—é—î, –ø–∞—Ä—Å–∏–º–æ HTML
            if not news_items:
                soup = BeautifulSoup(content, 'html.parser')
                articles = soup.find_all(['article', 'div'], class_=re.compile(r'post|article|entry|blog'))
                for article in articles[:5]:
                    title_elem = article.find(['h1', 'h2', 'h3'])
                    link_elem = article.find('a', href=True)
                    if title_elem and link_elem:
                        title = title_elem.get_text().strip()
                        link = urljoin(url, link_elem['href'])
                        content_elem = article.find(['p', 'div'])
                        content = content_elem.get_text().strip()[:500] if content_elem else title
                        news_items.append({
                            'title': title,
                            'content': content,
                            'url': link,
                            'source': urlparse(url).netloc,
                            'published': 'Recent'
                        })
        except Exception as e:
            logging.error(f"Parse error {url}: {e}")
        return news_items

    def fetch_reddit_posts(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ø–æ–≤–∏—Ö –ø–æ—Å—Ç—ñ–≤ –∑ AI subreddit'—ñ–≤"""
        subreddits = self.ai_config['sources']['reddit']
        posts = []
        
        for subreddit in subreddits:
            try:
                url = f"https://www.reddit.com/{subreddit}/hot.json?limit=5"
                headers = {'User-Agent': 'AI News Monitor 1.0'}
                response = requests.get(url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    for post in data['data']['children']:
                        post_data = post['data']
                        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ—Å—Ç–∏ –∑ —Ç–µ–∫—Å—Ç–æ–º/–ø–æ—Å–∏–ª–∞–Ω–Ω—è–º–∏
                        if post_data.get('selftext') or post_data.get('url'):
                            posts.append({
                                'title': post_data['title'],
                                'content': post_data.get('selftext', '')[:500],
                                'url': post_data.get('url', f"https://reddit.com{post_data['permalink']}"),
                                'source': f"reddit-{subreddit.replace('r/', '')}"
                            })
                time.sleep(1)  # –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
            except Exception as e:
                logging.error(f"–ü–æ–º–∏–ª–∫–∞ Reddit {subreddit}: {e}")
                continue
        
        return posts
    
    def search_ai_news(self):
        """–ü–æ—à—É–∫ AI –Ω–æ–≤–∏–Ω –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª"""
        logging.info("Searching for latest AI news...")
        all_news = []
        # –ü–∞—Ä—Å–∏–º–æ –±–ª–æ–≥–∏
        for blog_url in self.ai_config['sources']['blogs']:
            logging.info(f"–ü–∞—Ä—Å—é {blog_url}")
            news_items = self.scrape_blog_news(blog_url)
            all_news.extend(news_items)
            time.sleep(2)  # –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
        # –î–æ–¥–∞—î–º–æ Reddit
        reddit_posts = self.fetch_reddit_posts()
        all_news.extend(reddit_posts)
        # –î–æ–¥–∞—î–º–æ –ø–æ—à—É–∫ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ—à—É–∫ –¥–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –Ω–æ–≤–∏–Ω
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ requests –¥–ª—è –ø–æ—à—É–∫—É –Ω–æ–≤–∏–Ω —á–µ—Ä–µ–∑ Google News API –∞–±–æ —ñ–Ω—à—ñ –¥–∂–µ—Ä–µ–ª–∞
            search_queries = [
                "OpenAI new release",
                "Google AI breakthrough",
                "Microsoft AI news",
                "Anthropic Claude update",
                "AI product launch"
            ]
            # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–∂–µ—Ä–µ–ª–∞ –Ω–æ–≤–∏–Ω
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –≤–µ–±-–ø–æ—à—É–∫—É: {e}")
        logging.info(f"Found {len(all_news)} news items")
        return all_news
    
    async def search_ai_news_async(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –ø–æ—à—É–∫ –≤—Å—ñ—Ö –Ω–æ–≤–∏–Ω"""
        logging.info("Starting async news search...")
        async with aiohttp.ClientSession() as session:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏–º–æ –≤—Å—ñ –±–ª–æ–≥–∏
            blog_tasks = [self.scrape_blog_news_async(session, url)
                          for url in self.ai_config['sources']['blogs']]
            blog_results = await asyncio.gather(*blog_tasks, return_exceptions=True)
            all_news = []
            for result in blog_results:
                if isinstance(result, list):
                    all_news.extend(result)
        # Reddit –≤—Å–µ —â–µ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (—ó—Ö API –Ω–µ –ª—é–±–∏—Ç—å –±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤)
        reddit_posts = self.fetch_reddit_posts()
        all_news.extend(reddit_posts)
        logging.info(f"Found {len(all_news)} total news items (async)")
        return all_news
    
    def filter_news(self, news_list):
        """–§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑–∞ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏ —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å"""
        logging.info("Filtering news by criteria...")
        filtered = []
        keywords = self.ai_config['filter_criteria']['keywords']
        exclude_keywords = self.ai_config['filter_criteria'].get('exclude_keywords', [])
        for news in news_list:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –≤–∏–∫–ª—é—á–µ–Ω—ñ —Å–ª–æ–≤–∞
            text_to_check = f"{news['title']} {news['content']}".lower()
            if any(exclude_word.lower() in text_to_check for exclude_word in exclude_keywords):
                logging.info(f"Excluded: {news['title'][:50]}...")
                continue
            # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            if len(news['content']) < 100:
                continue
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–æ–≤–∏–Ω–∞ –≤–∂–µ –±—É–ª–∞ –Ω–∞–¥—ñ—Å–ª–∞–Ω–∞
            news_hash = self.get_news_hash(news['title'], news['url'])
            if news_hash in self.sent_news:
                continue
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø—Ä–∞—Ü—é—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è
            if not self.check_url_validity(news['url']):
                logging.warning(f"–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–µ –ø—Ä–∞—Ü—é—î: {news['url']}")
                continue
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
            if any(keyword.lower() in text_to_check for keyword in keywords):
                news['hash'] = news_hash
                filtered.append(news)
        logging.info(f"Found {len(filtered)} new relevant news")
        return filtered
    
    def translate_to_ukrainian(self, text):
        """–ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É"""
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "–¢–∏ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥–∞—á —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤. –ü–µ—Ä–µ–∫–ª–∞–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏ —Ç–∞ –Ω–∞–∑–≤–∏ –ø—Ä–æ–¥—É–∫—Ç—ñ–≤. –ü–µ—Ä–µ–∫–ª–∞–¥ –º–∞—î –±—É—Ç–∏ –ø—Ä–∏—Ä–æ–¥–Ω–∏–º —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª–∏–º."},
                    {"role": "user", "content": f"–ü–µ—Ä–µ–∫–ª–∞–¥–∏ —Ü–µ–π —Ç–µ–∫—Å—Ç –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É: {text}"}
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–∫–ª–∞–¥—É: {e}")
            return text
    
    def format_news_message(self, news):
        """–§–æ—Ä–º–∞—Ç—É—î–º–æ –Ω–æ–≤–∏–Ω—É –¥–ª—è Telegram"""
        
        # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
        title_ua = self.translate_to_ukrainian(news['title'])
        content_ua = self.translate_to_ukrainian(news['content'])
        
        # –§–æ—Ä–º–∞—Ç—É—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        message = f"üöÄ {title_ua}\n\n"
        
        # –î–æ–¥–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
        sentences = content_ua.split('. ')
        for sentence in sentences[:5]:  # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—ñ 5 —Ä–µ—á–µ–Ω–Ω—è
            if sentence.strip():
                message += f"‚Ä¢ {sentence.strip()}.\n"
        
        message += "\nüò∂üò∂üò∂\n\n"
        message += f"üîó –î–µ—Ç–∞–ª—å–Ω—ñ—à–µ: {news['url']}\n"
        message += f"üì∞ –î–∂–µ—Ä–µ–ª–æ: {news['source']}\n\n"
        message += "üëç [–ì—Ä—É–ø–∞](https://t.me/novyni_hi)"
        
        return message
    
    def send_to_telegram(self, message):
        """–ù–∞–¥—Å–∏–ª–∞—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —É Telegram"""
        bot_token = self.secrets['TELEGRAM']['secrets']['BOT_TOKEN']
        chat_id = self.telegram_config['target_group']['chat_id']
        
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        
        data = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'Markdown',
            'disable_web_page_preview': False
        }
        
        try:
            response = requests.post(url, data=data, timeout=10)
            if response.status_code == 200:
                logging.info("Message sent successfully!")
                return True
            else:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Telegram: {e}")
            return False
    
    def save_news_to_file(self, message, news):
        """–ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–æ–≤–∏–Ω—É —É —Ñ–∞–π–ª"""
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')
        filename = f"./data/news_{timestamp}.md"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# AI News - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            f.write(message)
            f.write(f"\n\n---\n")
            f.write(f"Original Title: {news['title']}\n")
            f.write(f"Source: {news['source']}\n")
            f.write(f"URL: {news['url']}\n")
        
        return filename
    
    def run_once(self):
        """–í–∏–∫–æ–Ω—É—î–º–æ –æ–¥–∏–Ω —Ü–∏–∫–ª –ø–æ—à—É–∫—É —Ç–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è –Ω–æ–≤–∏–Ω"""
        logging.info("Starting AI News Monitor...")
        
        try:
            # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –ø–æ—à—É–∫
            news_list = asyncio.run(self.search_ai_news_async())
            if not news_list:
                logging.info("No news found")
                return
            
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ
            filtered_news = self.filter_news(news_list)
            
            if not filtered_news:
                logging.info("No new relevant news found")
                return
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–Ω—É –Ω–æ–≤–∏–Ω—É (–º–∞–∫—Å–∏–º—É–º 3 –∑–∞ —Ä–∞–∑)
            sent_count = 0
            for news in filtered_news[:3]:
                try:
                    # –§–æ—Ä–º–∞—Ç—É—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
                    message = self.format_news_message(news)
                    
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —É —Ñ–∞–π–ª
                    filename = self.save_news_to_file(message, news)
                    logging.info(f"Message saved: {filename}")
                    
                    # –ù–∞–¥—Å–∏–ª–∞—î–º–æ —É Telegram
                    if self.send_to_telegram(message):
                        # –î–æ–¥–∞—î–º–æ –¥–æ —Å–ø–∏—Å–∫—É –Ω–∞–¥—ñ—Å–ª–∞–Ω–∏—Ö
                        self.sent_news.append(news['hash'])
                        sent_count += 1
                        
                        # –ü–∞—É–∑–∞ –º—ñ–∂ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è–º–∏
                        time.sleep(self.telegram_config['rate_limits']['delay_between_messages'])
                    
                except Exception as e:
                    logging.error(f"Error processing news: {e}")
                    continue
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å–ø–∏—Å–æ–∫ –Ω–∞–¥—ñ—Å–ª–∞–Ω–∏—Ö –Ω–æ–≤–∏–Ω
            self.save_sent_news()
            
            logging.info(f"Successfully sent {sent_count} news items")
            
        except Exception as e:
            logging.error(f"Critical error: {e}")
            import traceback
            traceback.print_exc()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    monitor = AINewsMonitor()
    monitor.run_once()

if __name__ == "__main__":
    main()